[{"item_id": 21095981, "text": "&gt; Load-sensitivity is one \u201csmart\u201d approach. The idea is that you keep track of the load on each shard, and selectively route traffic to the lightly-loaded ones and away from the busy ones. Simplest thing is, if you have some sort of load metric, always pick the shard with the lowest value.<p>Gotta be super careful with this one.  We did this at reddit and it bit us bad.  The problem was as soon as the load on a machine went down it got pounded with new requests and the load shot up, but it takes a few seconds for the load number to react to all the new requests.  So we saw really bad see-saw affect.<p>We had to add extra logic to mark how long a machine had beed at a certain load and also randomly send requests to slightly more loaded machines to keep things even.<p>The moral of the story here is make sure you pick a metric that reacts to the change in request rate as quickly as your request rate changes!", "by": "jedberg"}, {"item_id": 21096398, "text": "<i>Sorta</i> related:<p>I managed a team that built a 5x 1000 node distributed setup 10+ years ago.<p>We ended up going with<p>a) short DNS TTL + a custom DNS server that sent people to the closest cluster (with some intra-communication to avoid sending people to broken clusters)<p>b) in each cluster; three layers: 1) Linux keepalived load balancing, 2) Our custom HTTP&#x2F;TLS-level loadbalancers (~20 nodes per DC), 3) our application (~1000 nodes per DC)<p>A typical node had 24 (4x6) CPU cores when we started and 48 (4x12) towards the end.<p>These were not GC&#x2F;AWS nodes, we were buying hardware directly from IBM&#x2F;HP&#x2F;Dell&#x2F;AMD&#x2F;Intel&#x2F;SuperMicro and flying our own people out to mount them in DCs that we hired. Intel gave us some insane rebates when they were&#x27;re recovering from the AMD dominance.<p>Load-balancing policy: we just randomized targets, but kept sticky sessions. Nodes were stateless, except for shared app properties - we built a separate globally&#x2F;dc-aware \n distributed key-value store - that was a whole new thing 12 years ago we built based on the vague concept of AWS Dynamo. App nodes reported for duty to the load balancers when they were healthy.<p>We had a static country-to-preferred-DC mapping. That worked fine at this scale.<p>This setup worked fine for a decade and 250M+ MAUs. We had excellent availability.<p>At some point like 10 years ago a kinda well known US-based board member really, really wanted to us to move to AWS. So we did the cost calculations and realized it would cost like 8X more to host the service on AWS. That shut him up.<p>Different times. It&#x27;s so much easier now with AWS&#x2F;GC to build large-scale services. But also so much more expensive - still! I wonder how long that can last until the concept of dealing with computation, network and storage <i>really</i> becomes a commodity.", "by": "tpmx"}, {"item_id": 21096037, "text": "My favorite sharding&#x2F;load balancing algorithm is Highest Random Weight, or Rendezvous hashing [0].  It has all the benefits of consistent key hashing without the hotspots, and it doesn&#x27;t require any coordination between nodes.<p>[0] <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rendezvous_hashing\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rendezvous_hashing</a>", "by": "jedberg"}, {"item_id": 21095987, "text": "&gt; But the cache is a distraction. The performance you\u2019re going to get will depend on your record sizes and update patterns and anyhow you probabl don\u2019t care about the mean or median as much as the P99.<p>True your 99th percentile slowest requests won&#x27;t hit the cache, and certainly that caching won&#x27;t solve all your scaling difficulties.<p>However, keeping requests for commonly-needed data away from (say) a DB cluster decreases the load on it at a given level of throughput, and <i>that</i> can be good for P99, and (as the post notes) caching can specifically help with super-hot data which can cause problematic hotspots in some sharding strategies.<p>Obviously situations vary and there&#x27;re limits, but a cache seems like a legit tool, not just a band-aid, for a decent number of situations.", "by": "twotwotwo"}, {"item_id": 21095932, "text": "Another good strategy for load balancing&#x2F;sharding that always strikes me as simple but also devilishly cleaver is random pick two: <a href=\"https:&#x2F;&#x2F;brooker.co.za&#x2F;blog&#x2F;2012&#x2F;01&#x2F;17&#x2F;two-random.html\" rel=\"nofollow\">https:&#x2F;&#x2F;brooker.co.za&#x2F;blog&#x2F;2012&#x2F;01&#x2F;17&#x2F;two-random.html</a>", "by": "plandis"}, {"item_id": 21095313, "text": "Sounds more like load balancing than sharding.", "by": "oweiler"}, {"item_id": 21095524, "text": "Def worth clicking through to the shuffle sharding thread. Simple concept (and somewhat common in my experience) but I\u2019ve never seen the analysis before.", "by": "gfodor"}, {"item_id": 21099137, "text": "Is it just me, or is this article talking about load balancing, not sharding. My understanding of &quot;sharding&quot; is to split up a database into groups, either by time or by some index key (e.g., A-C on one shard, D-G on another, etc.). This article seems to be about splitting up web traffic, not sharding.", "by": "speedplane"}, {"item_id": 21099546, "text": "Is there any (significant) difference between sharding and load balancing?<p>It seems that in both cases the idea is to distribute (supposedly independent) requests between workers and one of the main difficulties is that requests might not be independent either within one stream (say, in the case of sessions) or between different streams (say, if they need to use one common state).", "by": "prostodata"}, {"item_id": 21097961, "text": "Wear a diaper?", "by": "KibbutzDalia"}, {"item_id": 21096692, "text": "My strategy is to get to the bathroom as soon as possible.", "by": "planetzero"}]