[{"item_id": 21109017, "text": "&gt; AlphaGo can play very well on a 19x19 board but actually has to be retrained to play on a rectangular board.<p>This right here is the soft underbelly of the entire \u201cmachine learning as step towards AGI\u201d hype machine, fueled in no small part by DeepMind and its flashy but misleading demos.<p>Once a human learns chess, you can give it a 10x10 board and she will perform at nearly the same skill level with zero retraining.<p>Give the same challenge to DeepMind\u2019s \u201csuperhuman\u201d game-playing machine and it will be an absolute patzer.<p>This is an obvious indicator that the state of the art in so-called \u201cmachine learning\u201d doesn\u2019t involve any actual learning in the way it is normally applied to intelligent systems like humans or animals.<p>I am continually amazed by the failure of otherwise exceedingly intelligent tech people to grasp this problem.", "by": "mindgam3"}, {"item_id": 21108264, "text": "I&#x27;m coming to suspect that even our data isn&#x27;t enough for useful AI. Imagine you had a truly general sci-fi AI at your office. It <i>still</i> couldn&#x27;t just look at your database and answer a simple question like &quot;What was the difference in client churn rates between mobile and desktop last month?&quot; or &quot;What was the effect of experiment 1234 on per-client revenue?&quot; Hell, a <i>human</i> couldn&#x27;t do it. As far as the human or AI would know, you just presented it with a bunch of random tables. This matters because it&#x27;s incredibly helpful to know which pieces are randomized. Which rows are repeated measurements as opposed to independent measurements. Which pieces are upstream of which others. There&#x27;s so much domain knowledge baked into data, while we just expect an algorithm to learn from a simple table of floats.<p>The human state of the art solution seems to be going on slack and asking questions about the data provenance, which will decidedly not work for an automated approach.<p>A primary reason I can do better a better job than a generic algorithm is because you told me where the data came from (or I designed the schema and ETL myself), while the algo can&#x27;t make any useful assumptions because all that info is hidden.", "by": "6gvONxR4sf7o"}, {"item_id": 21107521, "text": "I&#x27;ve been waiting for the Symbolic&#x2F;NN pendulum to starting swinging back the other way and start settling in the center. NN&#x2F;DL is great for the interface between the outer world and the inner world of the mind (pattern recognition and re-construction), and symbolic AI more straightforwardly represents more &quot;language of the mind&quot; tasks, and easily handles issues like explanation and other meta-behaviors that with DL is difficult due to its black-box nature. DL&#x27;s reliance on extension&#x2F;training vs. intention&#x2F;rules can develop ad-hoc intentional emergent theories which is their strength but also their weakness as these theories may not be correct or complete. Each can be brittle in their own way - so it&#x27;ll be interesting to see more cross-pollination.", "by": "thelazydogsback"}, {"item_id": 21107014, "text": "I started reading Rebooting AI last night. I think that Marcus and Davis (so far in the book) take a reasonable approach by wanting to design robust AI. Robust AI requires general real world intelligence that is not provided by deep learning.<p>I have earned over 90% of my income over the last five or six years as a deep learning practitioner. I am a fan of DL based on great results for perception tasks as well as solid NLP results like using BERT like models for things like anaphora  resolution.<p>But, I am in agreement with Marcus and Davis that our long term research priorities are wrong.", "by": "mark_l_watson"}, {"item_id": 21106940, "text": "The opposing argument, by Rich Sutton, Distinguished Research Scientist at DeepMind:<p><a href=\"http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html</a>", "by": "cs702"}, {"item_id": 21106947, "text": "<i>In cognitive science we talk about having cognitive models of things. So I\u2019m sitting in a hotel room, and I understand that there\u2019s a closet, there\u2019s a bed, there\u2019s the television that\u2019s mounted in an unusual way. I know that there are all these things here, and I don\u2019t just identify them. I also understand how they relate to one another. I have these ideas about how the outside world works. They\u2019re not perfect. They\u2019re fallible, but they\u2019re pretty good. And I make a lot of inferences around them to guide my everyday actions.</i><p><i>The opposite extreme is something like the Atari game system that DeepMind made, where it memorized what it needed to do as it saw pixels in particular places on the screen. If you get enough data, it can look like you\u2019ve got understanding, but it\u2019s actually a very shallow understanding. The proof is if you shift things by three pixels, it plays much more poorly. It breaks with the change. That\u2019s the opposite of deep understanding.</i><p>Of course. There are an infinte way to make interpretations of perceptions and a finite subset of possible valid ones.<p>It&#x27;s among those possible, that the AI will be a concrete implementation of an ideology.<p>To select which one is always done by humans.", "by": "sebastianconcpt"}, {"item_id": 21110864, "text": "I haven&#x27;t read the book, but the viewpoints he expresses in the interview are spot-on.  DL can a great alert&#x2F;suggestion mechanism in narrow domains, but it should never be trusted to make critical decisions.  I believe that general intelligence will only be achieved through major advancements in general symbolic reasoning.  Something like DL might play a small role in this breakthrough, but it will not be a core part of the solution.", "by": "1e-9"}, {"item_id": 21108877, "text": "&gt; <i>General AI also ought to be able to work just as comfortably reasoning about politics as reasoning about medicine. It\u2019s the analogue of what people have; any reasonably bright person can do many, many different things.</i><p>The average human has extreme difficulty reasoning about politics, while usually being reasonable on medicine (anti-vax being one of many exceptions). And it seems strange to expect a skilled pianist to also be a skilled neuroscientist or a skilled construction worker. On the other hand these people all use similar neural architectures (brains). So he seems pretty off-track when he criticizes &quot;narrow AI&quot; in favor of &quot;general AI&quot;, as if there&#x27;s some magic AI that will do everything perfectly, and even more off track when he criticizes researchers for using &quot;one-size-fits-all&quot; technologies, when indeed that is exactly what humans have been doing for millennia for their cognitive needs.<p>And sure, ML models in publications so far are typically one-off things that react poorly to modified inputs or unexpected situations. But it&#x27;s not clear this has any relevance to commercial use. Tesla is still selling self-driving cars despite the accidents.", "by": "Mathnerd314"}, {"item_id": 21111138, "text": "Gary Marcus literally gave a talk about this last week in my department&#x27;s ML seminar. I asked him how sure he was that humanity would eventually achieve AGI and he said 100%. When I asked him when that would be he replied 30-100 years. Interesting perspective.", "by": "gautamcgoel"}, {"item_id": 21107644, "text": "So, I\u2019ve been reading articles on this and I think I have a fuzzy idea of some of these solutions would entail. But what I\u2019m hung up on is this: if Deep learning is about coming up with solutions to problems that are too hard for humans, how do we hope to understand the rationale behind whatever solutions the machine comes up with?", "by": "gdubs"}, {"item_id": 21107809, "text": "One key question is whether symbolic AI is the right model of the world. It underperforms vector based AI on many specific tasks. But human experts heavily reply on it to communicate with each other. If symbolic AI is not the right model, P vs NP problem might be just irrelevant. Human philosophy is full of craps. We will lose a lot of beliefs.  \nElon will be right, we will abandon human languages, and connect through a cable in the brain. Everyone will relearn every thing from NN.<p>If symbolic AI is the right model, but difficult to build algorithmically. Vector based model just help to make it faster and better. Then we humans are fine. We simply proxy the lower level optimization to AI. Our functionalities will be shifted just like what happened when engine was invented hundreds of years ago.", "by": "XuMiao"}, {"item_id": 21108849, "text": "A classical AI company that&#x27;s working on &quot;a different part of the brain&quot;:<p><a href=\"https:&#x2F;&#x2F;www.cyc.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cyc.com&#x2F;</a>", "by": "brundolf"}, {"item_id": 21107102, "text": "<i>Understanding a sentence is fundamentally different from recognizing an object. But people are trying to use deep learning to do both.</i><p>I agree with most of the article but I think this^^ skips over the different types of networks used to solve perception and language problems.  A CNN is very different from say, word2vec, which isn&#x27;t a very deep network at all.", "by": "keithyjohnson"}, {"item_id": 21108797, "text": "Very curious about classical AI mixed with deep learning. Does anyone know of any examples?", "by": "aledalgrande"}, {"item_id": 21108236, "text": "We cannot trust NI (natural intelligence) systems built on mom-and-pop having sex alone", "by": "Iwan-Zotow"}, {"item_id": 21107431, "text": "In this context, what is classical AI?<p>An SVM? A markov model? A large context free grammar with a dictionary?", "by": "wodenokoto"}, {"item_id": 21107319, "text": null, "by": null}, {"item_id": 21108473, "text": "Why in earth would you trust AI alone ?  Same like driving my Tesla in deep sleep. So many times is been close encounters with obstacles..", "by": "fuguza"}, {"item_id": 21107822, "text": "Has work been done to formally prove general AI can&#x27;t arise from deep learning? I can&#x27;t help but feel its an assumption being made by those that prefer classical research.", "by": "jayd16"}, {"item_id": 21109904, "text": " I always found it fascinatingly ironic that AGI research only fund deep learning which is a local minimum.<p>People really interested in AGI should better look at Cyc and opencog", "by": "The_rationalist"}, {"item_id": 21107213, "text": "Why, yes. All life has the environmental constraints builtin in the structure of a brain and CNS.<p>Brain also has implicit sets of filters in sensory channels and in pattern-marching machinery.<p>Constraints and filters are the must have.", "by": "lngnmn1"}, {"item_id": 21107154, "text": "This article doesn\u2019t have any substance. It\u2019s full of anecdata like shifting by 3 pixels to mess up a video game AI or some vague nonsense about \u201ca model of this chair or this tv mounted to the wall.\u201d It\u2019s all casual hypotheticals.<p>There\u2019s plenty of research on Bayesian neural networks for causal inference. But even more, a lot of causal inference problems are \u201csmall data\u201d problems where choosing a strongly informative prior to pair with simple models is needed to prevent overfitting and poor generalization and to account for domain expertise.<p>Deep learning practitioners generally know plenty about this stuff and fully understand that deep neural networks are just one tool in the tool box, not applicable to all problems and certainly not approaching any kind of general AI solution that supersedes causal inference, feature engineering, etc.<p>This article is just a sensationalist hit job trying to capitalize on public anxieties about AI to raise the profile of this academic and try to sell more copies of his book.<p>I\u2019d say, let\u2019s not waste time on this crap. There are engineering problems that deep learning allows us to safely &amp; reliably solve where other methods never could. We absolutely can trust these models for specific use cases. Let\u2019s just get on with doing the work.", "by": "mlthoughts2018"}, {"item_id": 21107748, "text": "I have a crazy idea:<p>Make a system that allows one to scan their face, and OPT-OUT OF ALL FACIAL RECOGNITION.", "by": "samstave"}]