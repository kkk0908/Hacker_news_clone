[{"item_id": 21105493, "text": "Awesome, I love to see how there&#x27;s coming more and more frameworks for interpretability. It&#x27;s incredibly important, especially when selling your solution to higher-ups.<p>There&#x27;s another solution named LIME which seems to take a similar but more general approach, I like this more tailored idea as it&#x27;ll probably give a better interpretation of the NLP questions.", "by": "londogard"}, {"item_id": 21104512, "text": "Wow this is amazing. Check out their interactive demo that lets you see  visualizations for various tasks: <a href=\"https:&#x2F;&#x2F;demo.allennlp.org&#x2F;coreference-resolution&#x2F;MTA3MDQ4Nw==\" rel=\"nofollow\">https:&#x2F;&#x2F;demo.allennlp.org&#x2F;coreference-resolution&#x2F;MTA3MDQ4Nw=...</a><p>Interpretability is super important... first for technical debugging, and even more important for giving domain experts a view of the model inner working (otherwise they have to just trust the ML model blindly).", "by": "ivan_ah"}, {"item_id": 21104377, "text": "Slightly off topic, but the analogy of this to Deep Thought having to design a system to explain what the actual Question was is pretty amazing. Douglas Adams was incredibly prescient.", "by": "russellbeattie"}]